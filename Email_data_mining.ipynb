{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     -------------------------------------- 62.6/62.6 kB 480.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\developerintern5\\desktop\\webscrapping\\venv\\lib\\site-packages (4.12.2)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp311-cp311-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.7/96.7 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\developerintern5\\desktop\\webscrapping\\venv\\lib\\site-packages (from requests) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "     -------------------------------------- 123.2/123.2 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "     ------------------------------------ 157.0/157.0 kB 624.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\developerintern5\\desktop\\webscrapping\\venv\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: urllib3, et-xmlfile, charset-normalizer, certifi, requests, openpyxl\n",
      "Successfully installed certifi-2023.5.7 charset-normalizer-3.1.0 et-xmlfile-1.1.0 openpyxl-3.1.2 requests-2.31.0 urllib3-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import openpyxl\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Load the spreadsheet\n",
    "file_path = 'florida_companies.xlsx'\n",
    "workbook = openpyxl.load_workbook(file_path)\n",
    "sheet = workbook.active\n",
    "\n",
    "# Define the column index for the email addresses\n",
    "email_column = sheet.max_column + 1\n",
    "\n",
    "# Regex pattern to match email addresses\n",
    "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "\n",
    "# Maximum number of retries for failed requests\n",
    "max_retries = 3\n",
    "\n",
    "# Iterate over each row in the spreadsheet\n",
    "for row_index, row in enumerate(sheet.iter_rows(min_row=2, values_only=True), start=2):\n",
    "    url = row[2]  # Assuming the URLs are in the first column\n",
    "\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < max_retries and not success:\n",
    "        try:\n",
    "            # Send a GET request to the website\n",
    "            response = requests.get(url)\n",
    "            success = True\n",
    "\n",
    "        except RequestException as e:\n",
    "            print(f\"Request failed for URL: {url}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            retries += 1\n",
    "\n",
    "    if success:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find email addresses using regex\n",
    "        email_addresses = re.findall(email_pattern, soup.get_text())\n",
    "\n",
    "        # Join the found email addresses into a single string\n",
    "        email_string = ', '.join(email_addresses)\n",
    "\n",
    "        # Add the email addresses to the spreadsheet\n",
    "        sheet.cell(row=row_index, column=email_column).value = email_string\n",
    "    else:\n",
    "        print(f\"Skipping URL: {url}\")\n",
    "\n",
    "# Save the modified spreadsheet\n",
    "workbook.save(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
